{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Necessary libraries"
      ],
      "metadata": {
        "id": "nDcte_ZFqyHd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXAHmrXsZCYk"
      },
      "outputs": [],
      "source": [
        "import logging as log\n",
        "import math\n",
        "import itertools as it\n",
        "import numpy as np\n",
        "import scipy.special\n",
        "import scipy.stats\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import *\n",
        "%matplotlib inline\n",
        "import sys\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount Google Drive and copy data to `content`"
      ],
      "metadata": {
        "id": "VmKn1pjWq15B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gH5vlDkq4-I",
        "outputId": "f5cd3945-5e23-45b8-8651-33b72e620949"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy WEAT Data"
      ],
      "metadata": {
        "id": "NmRB-ikNLd0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! cp -R /content/drive/MyDrive/Contextual_Bias_Data/weat_bn_data /content/"
      ],
      "metadata": {
        "id": "4aJ9lv8BLfWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy SEAT Data"
      ],
      "metadata": {
        "id": "anp1dr4NvvzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! cp -R /content/drive/MyDrive/Contextual_Bias_Data/seat_bn_data /content/"
      ],
      "metadata": {
        "id": "1B-SaGEavxki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy **GloVe** model"
      ],
      "metadata": {
        "id": "C5cvCgCYLXP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# copy the data from drive folder to content folder\n",
        "! cp -R /content/drive/MyDrive/Contextual_Bias_Data/bn_glove.39M.300d.txt /content/"
      ],
      "metadata": {
        "id": "09ttNMews3sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy **word2vec** model"
      ],
      "metadata": {
        "id": "Vt5Mo1AnLqdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! cp -R /content/drive/MyDrive/Contextual_Bias_Data/bangla_embeddings/full_data/w2v_512 /content/"
      ],
      "metadata": {
        "id": "J-oOrBLXLp8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install `bnlp_toolkit` library for GloVe"
      ],
      "metadata": {
        "id": "YEGE4Y3KbzxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install bnlp_toolkit"
      ],
      "metadata": {
        "id": "mTTeQnjwbzZT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "065c10a0-5bfa-4f02-c944-c2e6be00dd75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: sentencepiece, python-crfsuite, emoji, sklearn-crfsuite, ftfy, bnlp_toolkit\n",
            "Successfully installed bnlp_toolkit-4.0.0 emoji-1.7.0 ftfy-6.1.1 python-crfsuite-0.9.9 sentencepiece-0.1.99 sklearn-crfsuite-0.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install `gensim` for **w2v** and **fasttext**"
      ],
      "metadata": {
        "id": "GnqlTfcnIZ4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install gensim\n",
        "# For loading \"bangla_embeddings/clean_subset/ft_512/ft_512\", gensim==3.8.3 is required"
      ],
      "metadata": {
        "id": "7b3bu4YhIfpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install dependencies for BERT"
      ],
      "metadata": {
        "id": "RAjiZGl00qPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install git+https://github.com/csebuetnlp/normalizer\n",
        "! pip install transformers"
      ],
      "metadata": {
        "id": "gYaoR6qh0puQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import BengaliGlove from bnlp_toolkit"
      ],
      "metadata": {
        "id": "s89MhR7S5OBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bnlp import BengaliGlove\n",
        "glove_path = '/content/bn_glove.39M.300d.txt'"
      ],
      "metadata": {
        "id": "YTc478xcb8we",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a817dc7-9f8c-4e04-f121-1fc58a26487b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "punkt not found. downloading...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "WARNING:root:The current version (3.3.2) of bnlp will not be compatible with the upcoming release.\n",
            "If you are using version <=3.3.2 please specify bnlp_toolkit with exact version, otherwise it will raises error in the upcoming version. \n",
            "To migrate feel free to checkout the newer version (4.0.0). It will release soon as beta.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test_model = BengaliGlove()\n",
        "# words =[\"মেয়ে\"]\n",
        "# for word in words:\n",
        "#     result = test_model.closest_word(glove_path, word)\n",
        "#     print(f'{word} --> {result}')"
      ],
      "metadata": {
        "id": "9ZtWkCpScTMN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3beebbc-42e5-4257-f022-11ca97639925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "মেয়ে --> ['মেয়ে', 'মেয়ের', 'ঘসেটি', 'জোসনা', 'বিবাহযোগ্য', 'ছেল', 'কন্যা', 'মায়ের', 'মনোয়ারা', 'বড়লোকের']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import **Word2Vec** from `gensim`"
      ],
      "metadata": {
        "id": "F9nBMqE5OLxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "o-qeUCK4OPuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fetch model from transformers"
      ],
      "metadata": {
        "id": "DQivXk2x03nw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "from normalizer import normalize\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/banglabert_large\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"csebuetnlp/banglabert_large\", output_hidden_states = True)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "oM4YWo4K07R-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement WEAT"
      ],
      "metadata": {
        "id": "P2u74uLT1A2j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Json Data"
      ],
      "metadata": {
        "id": "5JmsmTFO7U5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "# WEAT_SETS = [\"targ1\", \"targ2\", \"attr1\", \"attr2\"]\n",
        "# CATEGORY = \"category\"\n",
        "\n",
        "def load_json(sent_file):\n",
        "    ''' Load from json. We expect a certain format later, so do some post processing '''\n",
        "    print(f\"Loading {sent_file}...\")\n",
        "    all_data = json.load(open(sent_file, 'r'))\n",
        "    data = {}\n",
        "    targets = [ all_data['targ1']['category'], all_data['targ2']['category'] ]\n",
        "    attributes = [all_data['attr1']['category'], all_data['attr2']['category']]\n",
        "    for k, v in all_data.items():\n",
        "        examples = v[\"examples\"]\n",
        "        data[k] = examples\n",
        "        v[\"examples\"] = examples\n",
        "\n",
        "    return all_data, targets, attributes  # data"
      ],
      "metadata": {
        "id": "kGbAdD-lRCJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data, targets, attributes = load_json('/content/weat_bn_data/weat6b.jsonl')\n",
        "print(targets)\n",
        "print(attributes)\n",
        "# vector = model.word2vec(glove_path, data.targ1.examples[0])\n",
        "# len(vector)"
      ],
      "metadata": {
        "id": "4dZ_unURSer2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function encodes the words into a dictionary where -\n",
        "- `keys`: words\n",
        "- `values`: vector representations of words"
      ],
      "metadata": {
        "id": "iM81WYStlqFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(model, data, MODEL_NAME):\n",
        "    dict_word2vec = {}\n",
        "    if MODEL_NAME == 'glove':\n",
        "        for word in data:\n",
        "            vector = model.word2vec(glove_path, word)\n",
        "            dict_word2vec[word] = vector\n",
        "    elif MODEL_NAME == 'w2v':\n",
        "        for word in data:\n",
        "            vector = model.wv[word]\n",
        "            dict_word2vec[word] = vector\n",
        "\n",
        "    elif MODEL_NAME == 'bert':\n",
        "        # need to call getwordvector for 'count' sentences having same word, then store the mean of the 5 vectors in dict_word2vec[word]\n",
        "        for i, word in enumerate(data['words']):\n",
        "            # print(word)\n",
        "            itr = 0\n",
        "            vector_sum = np.zeros(1024, dtype=float)\n",
        "            while itr < data['count']:\n",
        "                # print(itr)\n",
        "                word_vector, _ = get_word_vector_normal(data['examples'][i*data['count'] + itr], word)\n",
        "                vector_sum = np.add(vector_sum, word_vector)\n",
        "                itr +=1\n",
        "            average_vector = vector_sum / data['count']\n",
        "            # Store the average vector in the dictionary\n",
        "            dict_word2vec[word] = average_vector\n",
        "\n",
        "    return dict_word2vec"
      ],
      "metadata": {
        "id": "ktDR87dQhk99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_word2vec = encode(model, all_seat_data[0]['targ1'], 'bert')\n",
        "type(dict_word2vec['গোলাপ'])"
      ],
      "metadata": {
        "id": "s9aZTJ3ZPeA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`encs_targ1` is a python dictionary having the word as **key** and the vector representation as **value**"
      ],
      "metadata": {
        "id": "RZ7Q85IMlcKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_data(model, data, MODEL_NAME, suppress_printables = False):\n",
        "    if suppress_printables == False:\n",
        "        print('encoding data...')\n",
        "    if MODEL_NAME == 'bert':\n",
        "        encs_targ1 = encode(model, data[\"targ1\"], MODEL_NAME)\n",
        "        encs_targ2 = encode(model, data[\"targ2\"], MODEL_NAME)\n",
        "        encs_attr1 = encode(model, data[\"attr1\"], MODEL_NAME)\n",
        "        encs_attr2 = encode(model, data[\"attr2\"], MODEL_NAME)\n",
        "    else:\n",
        "        encs_targ1 = encode(model, data[\"targ1\"][\"examples\"], MODEL_NAME)\n",
        "        encs_targ2 = encode(model, data[\"targ2\"][\"examples\"], MODEL_NAME)\n",
        "        encs_attr1 = encode(model, data[\"attr1\"][\"examples\"], MODEL_NAME)\n",
        "        encs_attr2 = encode(model, data[\"attr2\"][\"examples\"], MODEL_NAME)\n",
        "\n",
        "    return encs_targ1, encs_targ2, encs_attr1, encs_attr2"
      ],
      "metadata": {
        "id": "VN0P607-htfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function adds the vector encodings of the words to the **data dictionary**"
      ],
      "metadata": {
        "id": "Gaye0FdR8w_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_encodings_to_dict(data, encs_targ1, encs_targ2, encs_attr1, encs_attr2, suppress_printables = False):\n",
        "    if suppress_printables == False:\n",
        "        print('adding encoded vectors to data dict...')\n",
        "    data[\"targ1\"][\"encs\"] = encs_targ1\n",
        "    data[\"targ2\"][\"encs\"] = encs_targ2\n",
        "    data[\"attr1\"][\"encs\"] = encs_attr1\n",
        "    data[\"attr2\"][\"encs\"] = encs_attr2"
      ],
      "metadata": {
        "id": "wEojlJZvi8k1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saves the encodings of `glove` to a file for caching ( GloVe takes a lot of time to encode a word )"
      ],
      "metadata": {
        "id": "NtKkOUjS9Euc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CATEGORIES = ['targ1', 'targ2', 'attr1', 'attr2']\n",
        "def save_encodings(data, filename):\n",
        "    for category in CATEGORIES:\n",
        "        for word, vec in data[category]['encs'].items():\n",
        "            data[category]['encs'][word] = data[category]['encs'][word].tolist()\n",
        "    with open('/content/weat_bn_encoded_data/weat8b_enc.jsonl', 'w') as f:\n",
        "        json.dump(data, f)"
      ],
      "metadata": {
        "id": "0YrVK2_nOJI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> `X` and `Y` are two sets of target words of equal size\n",
        "\n",
        "> `A` and `B` are two sets of attribute words\n",
        "\n",
        "$s(X, Y, A, B) = [\\mathcal{E}_{x∈X}s(x, A, B)− \\mathcal{E}_{y∈Y}s(y, A, B)]$\n",
        "\n",
        "$s(w, A, B) = [mean_{a∈A}\\cos(w, a)− mean_{b∈B}\\cos(w, b)]$\n",
        "\n",
        "$d = \\frac{mean_{x∈X}s(x, A, B) - mean_{y∈Y}s(y, A, B)}{std\\_dev_{w∈X \\cup Y}s(w, A, B)}$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_m8ihcMXZ4u_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Calculate similarity scores -"
      ],
      "metadata": {
        "id": "z3e6o62V44PM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(x, y):\n",
        "    return np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))"
      ],
      "metadata": {
        "id": "o4zWllQVZwBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def construct_cossim_lookup(XY, AB):\n",
        "    \"\"\"\n",
        "    XY: mapping from target string to target vector (either in X or Y)\n",
        "    AB: mapping from attribute string to attribute vectore (either in A or B)\n",
        "    Returns an array of size (len(XY), len(AB)) containing cosine similarities\n",
        "    between items in XY and items in AB.\n",
        "    \"\"\"\n",
        "\n",
        "    cossims = np.zeros((len(XY), len(AB)))\n",
        "    for xy in XY:\n",
        "        for ab in AB:\n",
        "            cossims[xy, ab] = cosine_similarity(XY[xy], AB[ab])\n",
        "    return cossims"
      ],
      "metadata": {
        "id": "vXys6VdTaM02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def s_wAB(A, B, cossims):\n",
        "    \"\"\"\n",
        "    Return vector of s(w, A, B) across w, where\n",
        "        s(w, A, B) = mean_{a in A} cos(w, a) - mean_{b in B} cos(w, b).\n",
        "    \"\"\"\n",
        "    return cossims[:, A].mean(axis=1) - cossims[:, B].mean(axis=1)"
      ],
      "metadata": {
        "id": "JOki05yMqwVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def s_XAB(X, s_wAB_memo):\n",
        "    return s_wAB_memo[X].sum()\n",
        "\n",
        "def s_XYAB(X, Y, s_wAB_memo):\n",
        "    return s_XAB(X, s_wAB_memo) - s_XAB(Y, s_wAB_memo)"
      ],
      "metadata": {
        "id": "Tjl7f6z60ZQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Permutation Test"
      ],
      "metadata": {
        "id": "oVSCtzt50mYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def p_val_permutation_test(X, Y, A, B, n_samples, cossims, parametric=False, suppress_printables = False):\n",
        "    ''' Compute the p-val for the permutation test, which is defined as\n",
        "        the probability that a random even partition X_i, Y_i of X u Y\n",
        "        satisfies P[s(X_i, Y_i, A, B) > s(X, Y, A, B)]\n",
        "    '''\n",
        "    X = np.array(list(X), dtype=int)\n",
        "    Y = np.array(list(Y), dtype=int)\n",
        "    A = np.array(list(A), dtype=int)\n",
        "    B = np.array(list(B), dtype=int)\n",
        "\n",
        "    assert len(X) == len(Y)\n",
        "    size = len(X)\n",
        "    s_wAB_memo = s_wAB(A, B, cossims=cossims)\n",
        "    XY = np.concatenate((X, Y))\n",
        "\n",
        "    if parametric:\n",
        "        if suppress_printables == False:\n",
        "            print('Using parametric test')\n",
        "        s = s_XYAB(X, Y, s_wAB_memo)\n",
        "\n",
        "        if suppress_printables == False:\n",
        "            print('Drawing {} samples'.format(n_samples))\n",
        "        samples = []\n",
        "        for _ in range(n_samples):\n",
        "            np.random.shuffle(XY)\n",
        "            Xi = XY[:size]\n",
        "            Yi = XY[size:]\n",
        "            assert len(Xi) == len(Yi)\n",
        "            si = s_XYAB(Xi, Yi, s_wAB_memo)\n",
        "            samples.append(si)\n",
        "\n",
        "        # Compute sample standard deviation and compute p-value by\n",
        "        # assuming normality of null distribution\n",
        "        if suppress_printables == False:\n",
        "            print('Inferring p-value based on normal distribution')\n",
        "        (shapiro_test_stat, shapiro_p_val) = scipy.stats.shapiro(samples)\n",
        "        if suppress_printables == False:\n",
        "            print('Shapiro-Wilk normality test statistic: {:.2g}, p-value: {:.2g}'.format(\n",
        "            shapiro_test_stat, shapiro_p_val))\n",
        "        sample_mean = np.mean(samples)\n",
        "        sample_std = np.std(samples, ddof=1)\n",
        "        if suppress_printables == False:\n",
        "            print('Sample mean: {:.2g}, sample standard deviation: {:.2g}'.format(\n",
        "            sample_mean, sample_std))\n",
        "        p_val = scipy.stats.norm.sf(s, loc=sample_mean, scale=sample_std)\n",
        "        return p_val\n",
        "\n",
        "    else:\n",
        "        if suppress_printables == False:\n",
        "            print('Using non-parametric test')\n",
        "        s = s_XAB(X, s_wAB_memo)\n",
        "        total_true = 0\n",
        "        total_equal = 0\n",
        "        total = 0\n",
        "\n",
        "        num_partitions = int(scipy.special.binom(2 * len(X), len(X)))\n",
        "        if num_partitions > n_samples:\n",
        "            # We only have as much precision as the number of samples drawn;\n",
        "            # bias the p-value (hallucinate a positive observation) to\n",
        "            # reflect that.\n",
        "            total_true += 1\n",
        "            total += 1\n",
        "            if suppress_printables == False:\n",
        "                print('Drawing {} samples (and biasing by 1)'.format(n_samples - total))\n",
        "            for _ in range(n_samples - 1):\n",
        "                np.random.shuffle(XY)\n",
        "                Xi = XY[:size]\n",
        "                assert 2 * len(Xi) == len(XY)\n",
        "                si = s_XAB(Xi, s_wAB_memo)\n",
        "                if si > s:\n",
        "                    total_true += 1\n",
        "                elif si == s:  # use conservative test\n",
        "                    total_true += 1\n",
        "                    total_equal += 1\n",
        "                total += 1\n",
        "\n",
        "        else:\n",
        "            if suppress_printables == False:\n",
        "                print('Using exact test ({} partitions)'.format(num_partitions))\n",
        "            for Xi in it.combinations(XY, len(X)):\n",
        "                Xi = np.array(Xi, dtype=\n",
        "                              int)\n",
        "                assert 2 * len(Xi) == len(XY)\n",
        "                si = s_XAB(Xi, s_wAB_memo)\n",
        "                if si > s:\n",
        "                    total_true += 1\n",
        "                elif si == s:  # use conservative test\n",
        "                    total_true += 1\n",
        "                    total_equal += 1\n",
        "                total += 1\n",
        "\n",
        "        if total_equal:\n",
        "            if suppress_printables == False:\n",
        "                print('Equalities contributed {}/{} to p-value'.format(total_equal, total))\n",
        "\n",
        "        return total_true / total\n",
        "\n"
      ],
      "metadata": {
        "id": "uDIAfMld0kke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate `mean` and `standard deviation`"
      ],
      "metadata": {
        "id": "fwNQ0mlWM5Wd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_s_wAB(X, A, B, cossims):\n",
        "    return np.mean(s_wAB(A, B, cossims[X]))\n",
        "\n",
        "def stdev_s_wAB(X, A, B, cossims):\n",
        "    return np.std(s_wAB(A, B, cossims[X]), ddof=1)"
      ],
      "metadata": {
        "id": "DSHXSe1-M30c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate **effect size**"
      ],
      "metadata": {
        "id": "5V7UUqI6NBBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def effect_size(X, Y, A, B, cossims):\n",
        "    \"\"\"\n",
        "    Compute the effect size, which is defined as\n",
        "        [mean_{x in X} s(x, A, B) - mean_{y in Y} s(y, A, B)] /\n",
        "            [ stddev_{w in X u Y} s(w, A, B) ]\n",
        "    args:\n",
        "        - X, Y, A, B : sets of target (X, Y) and attribute (A, B) indices\n",
        "    \"\"\"\n",
        "    X = list(X)\n",
        "    Y = list(Y)\n",
        "    A = list(A)\n",
        "    B = list(B)\n",
        "\n",
        "    numerator = mean_s_wAB(X, A, B, cossims=cossims) - mean_s_wAB(Y, A, B, cossims=cossims)\n",
        "    denominator = stdev_s_wAB(X + Y, A, B, cossims=cossims)\n",
        "    return numerator / denominator"
      ],
      "metadata": {
        "id": "psQ8_w-9gqpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_keys_to_ints(X, Y):\n",
        "    return (\n",
        "        dict((i, v) for (i, (k, v)) in enumerate(X.items())),\n",
        "        dict((i + len(X), v) for (i, (k, v)) in enumerate(Y.items())),\n",
        "    )"
      ],
      "metadata": {
        "id": "qZCPltJdNNeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_test(encs, n_samples, parametric=False, suppress_printables = False):\n",
        "    ''' Run a WEAT.\n",
        "    args:\n",
        "        - encs (Dict[str: Dict]): dictionary mapping targ1, targ2, attr1, attr2\n",
        "            to dictionaries containing the category and the encodings\n",
        "        - n_samples (int): number of samples to draw to estimate p-value\n",
        "            (use exact test if number of permutations is less than or\n",
        "            equal to n_samples)\n",
        "    '''\n",
        "    X, Y = encs[\"targ1\"][\"encs\"], encs[\"targ2\"][\"encs\"]\n",
        "    A, B = encs[\"attr1\"][\"encs\"], encs[\"attr2\"][\"encs\"]\n",
        "\n",
        "    # First convert all keys to ints to facilitate array lookups\n",
        "    (X, Y) = convert_keys_to_ints(X, Y)\n",
        "    (A, B) = convert_keys_to_ints(A, B)\n",
        "\n",
        "    XY = X.copy()\n",
        "    XY.update(Y)\n",
        "    AB = A.copy()\n",
        "    AB.update(B)\n",
        "\n",
        "    if suppress_printables == False:\n",
        "        print(\"Computing cosine similarities...\")\n",
        "    cossims = construct_cossim_lookup(XY, AB)\n",
        "\n",
        "    if suppress_printables == False:\n",
        "        print(f\"Null hypothesis: no difference between {encs['targ1']['category']} and {encs['targ2']['category']} in \\\n",
        "    association to attributes {encs['attr1']['category']} and {encs['attr2']['category']}\")\n",
        "        print(\"Computing pval...\")\n",
        "    pval = p_val_permutation_test(X, Y, A, B, n_samples, cossims=cossims, parametric=parametric, suppress_printables=suppress_printables)\n",
        "    print(f\"pval: {pval}\")\n",
        "\n",
        "    if suppress_printables == False:\n",
        "        print(\"computing effect size...\")\n",
        "    esize = effect_size(X, Y, A, B, cossims=cossims)\n",
        "    print(f\"Effect size: {esize}\")\n",
        "    return esize, pval\n"
      ],
      "metadata": {
        "id": "ePktHsGKNUOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Demo cell to test functionality"
      ],
      "metadata": {
        "id": "etcF18Zz9Zwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = {\"x\" + str(i): 2 * np.random.rand(10) - 1 for i in range(25)}\n",
        "Y = {\"y\" + str(i): 2 * np.random.rand(10) - 1 for i in range(25)}\n",
        "A = {\"a\" + str(i): 2 * np.random.rand(10) - 1 for i in range(25)}\n",
        "B = {\"b\" + str(i): 2 * np.random.rand(10) - 1 for i in range(25)}\n",
        "A = X\n",
        "B = Y\n",
        "print(f'X: {X}')\n",
        "print(f'Y: {Y}')\n",
        "(X, Y) = convert_keys_to_ints(X, Y)\n",
        "print(f'X: {X}')\n",
        "print(f'Y: {Y}')\n",
        "(A, B) = convert_keys_to_ints(A, B) # converts the keys (string) to keys (integer)\n",
        "\n",
        "XY = X.copy()\n",
        "XY.update(Y)\n",
        "AB = A.copy()\n",
        "AB.update(B)\n",
        "\n",
        "cossims = construct_cossim_lookup(XY, AB)\n",
        "print(\"computing pval...\")\n",
        "pval = p_val_permutation_test(X, Y, A, B, cossims=cossims, n_samples=10000)\n",
        "print(\"pval: %g\", pval)\n",
        "\n",
        "print(\"computing effect size...\")\n",
        "esize = effect_size(X, Y, A, B, cossims=cossims)\n",
        "print(f\"Effect size: {esize}\")"
      ],
      "metadata": {
        "id": "ORDs2deTNLBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load intended Model"
      ],
      "metadata": {
        "id": "pCQV91s_27ZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"glove\"\n",
        "\n",
        "if MODEL_NAME == 'w2v':\n",
        "    print('Loading w2v model...')\n",
        "    model = Word2Vec.load(\"/content/w2v_512/w2v_512\")\n",
        "elif MODEL_NAME == 'glove':\n",
        "    print('Loading glove model...')\n",
        "    model = BengaliGlove()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPXwRc0m24LQ",
        "outputId": "e20631d3-48bd-4f00-d823-cdcaa8419483"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading glove model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WEAT on single data"
      ],
      "metadata": {
        "id": "cBPo7RuF2q0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = '/content/weat_bn_data/weat8b.jsonl'\n",
        "data, targets, attributes = load_json(filename)\n",
        "\n",
        "et1, et2, ea1, ea2 = encode_data(model, data, MODEL_NAME, suppress_printables = False)\n",
        "add_encodings_to_dict(data, et1, et2, ea1, ea2, suppress_printables = False)\n",
        "save_encodings(data, filename)\n",
        "run_test(data, 10000, suppress_printables = False) # X, Y need to have same length for p_val permutation test\n",
        "\n",
        "print(f\"{targets[0]} vs {targets[1]} ({attributes[0]} / {attributes[1]})\")"
      ],
      "metadata": {
        "id": "2Cn9DLbNsIVG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d163bbfd-fc43-4a4d-f70e-f4f942fe732b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading /content/weat_bn_data/weat8b.jsonl...\n",
            "encoding data...\n",
            "adding encoded vectors to data dict...\n",
            "Using non-parametric test\n",
            "Drawing 9999 samples (and biasing by 1)\n",
            "pval: 0.9991\n",
            "Effect size: -1.0394699306880417\n",
            "Science vs Arts (MaleNames / FemaleNames)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load saved encoding and re-run GloVe Test"
      ],
      "metadata": {
        "id": "vnaY0MZL9kr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_enc = json.load(open('/content/weat_bn_encoded_data/weat8b_enc.jsonl', 'r'))\n",
        "run_test(data, 10000, parametric = True, suppress_printables = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bFVxaP4y3e3",
        "outputId": "1764e540-d05b-4280-e729-bc80c7d9d00a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using parametric test\n",
            "Drawing 10000 samples\n",
            "Inferring p-value based on normal distribution\n",
            "Shapiro-Wilk normality test statistic: 1, p-value: 0.49\n",
            "Sample mean: 0.0077, sample standard deviation: 0.38\n",
            "pval: 0.998898652803399\n",
            "Effect size: -1.0394699306880417\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_morestats.py:1816: UserWarning: p-value may not be accurate for N > 5000.\n",
            "  warnings.warn(\"p-value may not be accurate for N > 5000.\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-1.0394699306880417, 0.998898652803399)"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# ! zip -r /content/wn_bn_data_encoded.zip /content/weat_bn_encoded_data/\n",
        "# files.download('/content/wn_bn_data_encoded.zip')"
      ],
      "metadata": {
        "id": "A6GHeW_nRmc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "WEAT on whole dataset"
      ],
      "metadata": {
        "id": "qNufZXgr8oOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = '/content/weat_bn_data'\n",
        "files = os.listdir(folder_path)\n",
        "\n",
        "for file in files:\n",
        "    file_path = os.path.join(folder_path, file)\n",
        "    if os.path.isfile(file_path):\n",
        "        # print(f\"filepath -> {file_path}\")\n",
        "        data, targets, attributes = load_json(file_path)\n",
        "        et1, et2, ea1, ea2 = encode_data(model, data, MODEL_NAME, suppress_printables = True)\n",
        "        add_encodings_to_dict(data, et1, et2, ea1, ea2, suppress_printables = True)\n",
        "\n",
        "        print(f\"{targets[0]} vs {targets[1]} ({attributes[0]} / {attributes[1]})\")\n",
        "        run_test(data, 10000, suppress_printables = True)\n",
        "        print()\n"
      ],
      "metadata": {
        "id": "eSzkdO5L8Ckb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement WEAT for BERT"
      ],
      "metadata": {
        "id": "L7w3mYIc2vuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_vector_broken(sentence, word):\n",
        "  normalized_sent = normalize(sentence)\n",
        "#   print(normalized_sent)\n",
        "  input_token = tokenizer(normalized_sent, return_tensors=\"pt\")\n",
        "#   print(input_token)\n",
        "  sent_list = sentence.split(' ')\n",
        "  idx = sent_list.index(word) + 1 # for [CLS]\n",
        "#   print(f'{sentence} \\n {word} -- {idx}')\n",
        "  with torch.no_grad():\n",
        "    outputs = model(**input_token)\n",
        "    # print(outputs.hidden_states[-1].shape) (1, 8, 1024)\n",
        "    print(outputs.hidden_states[-1][0][idx])\n",
        "    # return outputs[1][24][0][idx]\n",
        "    return outputs[1][-1][0].detach().cpu().numpy()[idx]# + outputs[1][-1][0].detach().cpu().numpy()[idx + 1] + outputs[1][-1][0].detach().cpu().numpy()[idx + 2]"
      ],
      "metadata": {
        "id": "Cu92nhHvz9uT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"ছেলেরা বিকেলে মাঠে ফুটবল খেলে।\"\n",
        "get_word_vector(sentence, 'খেলে')"
      ],
      "metadata": {
        "id": "mUg5qLs_0bZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test `get_word_vector`"
      ],
      "metadata": {
        "id": "TwDBCCcS3sQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def get_word_vector_normal(sentence, word):\n",
        "\n",
        "    normalized_sent = normalize(sentence)\n",
        "    word = normalize(word)\n",
        "    # print(f\"normalized: {normalized_sent}\")\n",
        "    input_token_mappings = tokenizer(normalized_sent, return_tensors=\"pt\", return_offsets_mapping = True)\n",
        "    input_token = tokenizer(normalized_sent, return_tensors=\"pt\")\n",
        "    # print(f\"tokens: {input_token_mappings}\")\n",
        "    decoded = tokenizer.decode(input_token['input_ids'][0])\n",
        "    # print(f\"Decoded tokens: {decoded}\")\n",
        "    sent_list = normalized_sent.split(' ')\n",
        "    # print(f\"sentence list: {sent_list}\")\n",
        "    if word in sent_list:\n",
        "        idx = sent_list.index(word) + 1\n",
        "    else:\n",
        "        pattern = r'\\b' + word + r'\\W*'\n",
        "        # print(pattern)\n",
        "        for i, w in enumerate(sent_list):\n",
        "            if re.search(pattern, w):\n",
        "                # print(\"found\")\n",
        "                idx = i + 1\n",
        "    # print(f'{sentence} -> {word}({idx})')\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**input_token)\n",
        "        # print(type(outputs[1][24][0]))\n",
        "        # print(len(outputs[1][24][0]))\n",
        "        # print(idx)\n",
        "        return outputs.hidden_states[-1][0].detach().cpu().numpy()[idx], input_token_mappings"
      ],
      "metadata": {
        "id": "zEp5Dhy_yLFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"এটি একটি কৃষ্ণচূড়া।\"\n",
        "print(get_word_vector_normal(sentence, 'কৃষ্ণচূড়া'))\n",
        "# len(get_word_vector(sentence, 'ফুটবল'))"
      ],
      "metadata": {
        "id": "bEchJQkpyPwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check [this](https://colab.research.google.com/drive/1RZgGPBSIdnMnDhr2r9Ov3W_6exbL_wOQ#scrollTo=EjZvVx7YKseh) notebook for problematic tokens"
      ],
      "metadata": {
        "id": "rd3h8DDmYtUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"বাগানে রজনীগন্ধা ফুল ফুটেছে।\"\n",
        "sentence2 = \"বাগানে গোলাপ ফুল ফুটেছে।\"\n",
        "vector1 = get_word_vector_broken(sentence, 'রজনীগন্ধা')\n",
        "vector2, _ = get_word_vector_normal(sentence2, 'গোলাপ')\n",
        "vector2.shape\n",
        "# cosine_similarity(vector1, vector2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VsA-GJUUPk-",
        "outputId": "852043ce-e95e-4f2e-a8a0-0d47e4586447"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.1118,  0.3700,  0.7958,  ...,  0.4475,  1.1334, -0.2145])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1024,)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "# WEAT_SETS = [\"targ1\", \"targ2\", \"attr1\", \"attr2\"]\n",
        "# CATEGORY = \"category\"\n",
        "\n",
        "def load_json_templates(sent_file):\n",
        "    ''' Load from json. We expect a certain format later, so do some post processing '''\n",
        "    print(f\"Loading {sent_file}...\")\n",
        "    all_data = json.load(open(sent_file, 'r'))\n",
        "    data = {}\n",
        "    targets = [ all_data['targ1']['category'], all_data['targ2']['category'] ]\n",
        "    attributes = [all_data['attr1']['category'], all_data['attr2']['category']]\n",
        "    for k, v in all_data.items():\n",
        "        templates = v[\"templates\"]\n",
        "        data[k] = templates\n",
        "        v[\"templates\"] = templates\n",
        "\n",
        "    return all_data, targets, attributes  # data"
      ],
      "metadata": {
        "id": "A5AB7dOY7kPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = '/content/seat_bn_data/templates'\n",
        "files = os.listdir(folder_path)\n",
        "all_seat_template_data = []\n",
        "files.sort()\n",
        "for file in files:\n",
        "    file_path = os.path.join(folder_path, file)\n",
        "    if os.path.isfile(file_path):\n",
        "        # print(f\"filepath -> {file_path}\")\n",
        "        seat_template_data, targets, attributes = load_json_templates(file_path)\n",
        "        all_seat_template_data.append(seat_template_data)\n",
        "        # print(data['targ1']['category'])\n",
        "        print(f\"{seat_template_data['targ1']['category']} has {len(seat_template_data['targ1']['templates'])} sentences\")\n",
        "        print(f\"{seat_template_data['targ2']['category']} has {len(seat_template_data['targ2']['templates'])} sentences\")\n",
        "        print(f\"{seat_template_data['attr1']['category']} has {len(seat_template_data['attr1']['templates'])} sentences\")\n",
        "        print(f\"{seat_template_data['attr2']['category']} has {len(seat_template_data['attr2']['templates'])} sentences\")"
      ],
      "metadata": {
        "id": "d39j94-G7vy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weat_folder_path = '/content/weat_bn_data'\n",
        "weat_files = os.listdir(weat_folder_path)\n",
        "weat_files.sort()\n",
        "all_weat_data = []\n",
        "\n",
        "for file in weat_files:\n",
        "    weat_file_path = os.path.join(weat_folder_path, file)\n",
        "    if os.path.isfile(weat_file_path):\n",
        "        # print(f\"filepath -> {file_path}\")\n",
        "        weat_data, targets, attributes = load_json(weat_file_path)\n",
        "        all_weat_data.append(weat_data)\n",
        "        # print(data['targ1']['category'])\n",
        "        print(f\"{weat_data['targ1']['category']} has {len(weat_data['targ1']['examples'])} words\")\n",
        "        print(f\"{weat_data['targ2']['category']} has {len(weat_data['targ2']['examples'])} words\")\n",
        "        print(f\"{weat_data['attr1']['category']} has {len(weat_data['attr1']['examples'])} words\")\n",
        "        print(f\"{weat_data['attr2']['category']} has {len(weat_data['attr2']['examples'])} words\")"
      ],
      "metadata": {
        "id": "GeeamG_M_T30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, data in enumerate(all_weat_data):\n",
        "    data['targ1']['count'] = len(all_seat_template_data[i]['targ1']['templates'])\n",
        "    data['targ2']['count'] = len(all_seat_template_data[i]['targ2']['templates'])\n",
        "    data['attr1']['count'] = len(all_seat_template_data[i]['attr1']['templates'])\n",
        "    data['attr2']['count'] = len(all_seat_template_data[i]['attr2']['templates'])"
      ],
      "metadata": {
        "id": "uyoMfXzkBSs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`all_weat_data` now has an additional key-value pair which contains the number of template sentences for each target/attribute\n",
        "\n",
        "print `all_weat_data[0]` for clarification"
      ],
      "metadata": {
        "id": "ytDdgVrEDce1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_seat_data[0]"
      ],
      "metadata": {
        "id": "v0-MnxQ4DKQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = '/content/seat_bn_data/data'\n",
        "files = os.listdir(folder_path)\n",
        "all_seat_data = []\n",
        "files.sort()\n",
        "for file in files:\n",
        "    file_path = os.path.join(folder_path, file)\n",
        "    if os.path.isfile(file_path):\n",
        "        # print(f\"filepath -> {file_path}\")\n",
        "        seat_data, targets, attributes = load_json(file_path)\n",
        "        all_seat_data.append(seat_data)\n",
        "        # print(data['targ1']['category'])\n",
        "        print(f\"{seat_data['targ1']['category']} has {len(seat_data['targ1']['examples'])} sentences\")\n",
        "        print(f\"{seat_data['targ2']['category']} has {len(seat_data['targ2']['examples'])} sentences\")\n",
        "        print(f\"{seat_data['attr1']['category']} has {len(seat_data['attr1']['examples'])} sentences\")\n",
        "        print(f\"{seat_data['attr2']['category']} has {len(seat_data['attr2']['examples'])} sentences\")"
      ],
      "metadata": {
        "id": "WHxoztytCqlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Need to use `all_weat_data` and `all_seat_sentences` to identify which word's vector should be fetched"
      ],
      "metadata": {
        "id": "MdqVa-yVDu03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, seat_data in enumerate(all_seat_data):\n",
        "    seat_data['targ1']['words'] = all_weat_data[i]['targ1']['examples']\n",
        "    seat_data['targ1']['count'] = all_weat_data[i]['targ1']['count']\n",
        "\n",
        "    seat_data['targ2']['words'] = all_weat_data[i]['targ2']['examples']\n",
        "    seat_data['targ2']['count'] = all_weat_data[i]['targ2']['count']\n",
        "\n",
        "    seat_data['attr1']['words'] = all_weat_data[i]['attr1']['examples']\n",
        "    seat_data['attr1']['count'] = all_weat_data[i]['attr1']['count']\n",
        "\n",
        "    seat_data['attr2']['words'] = all_weat_data[i]['attr2']['examples']\n",
        "    seat_data['attr2']['count'] = all_weat_data[i]['attr2']['count']"
      ],
      "metadata": {
        "id": "MzKEYtMeDIaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do a sanity check ✔"
      ],
      "metadata": {
        "id": "YNZ5csqHKa3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, seat_data in enumerate(all_seat_data):\n",
        "    if len(seat_data['targ1']['words'])*seat_data['targ1']['count'] != len(seat_data['targ1']['examples']):\n",
        "        print(\"Issue\")\n",
        "    if len(seat_data['targ2']['words'])*seat_data['targ2']['count'] != len(seat_data['targ2']['examples']):\n",
        "        print(\"Issue\")\n",
        "    if len(seat_data['attr1']['words'])*seat_data['attr1']['count'] != len(seat_data['attr1']['examples']):\n",
        "        print(\"Issue\")\n",
        "    if len(seat_data['attr2']['words'])*seat_data['attr2']['count'] != len(seat_data['attr2']['examples']):\n",
        "        print(\"Issue\")"
      ],
      "metadata": {
        "id": "F8LvIOzlKaM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SEAT on single file"
      ],
      "metadata": {
        "id": "cZ7HkNflShe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'bert'\n",
        "\n",
        "et1, et2, ea1, ea2 = encode_data(model, all_seat_data[0], MODEL_NAME, suppress_printables = False)\n",
        "add_encodings_to_dict(all_seat_data[0], et1, et2, ea1, ea2, suppress_printables = False)\n",
        "run_test(all_seat_data[0], 10000, suppress_printables = False) # X, Y need to have same length for p_val permutation test"
      ],
      "metadata": {
        "id": "BkvSeLb_KRO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SEAT on whole dataset"
      ],
      "metadata": {
        "id": "fnW0GwO0W6K7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'bert'\n",
        "for seat_data in all_seat_data:\n",
        "    et1, et2, ea1, ea2 = encode_data(model, seat_data, MODEL_NAME, suppress_printables = False)\n",
        "    add_encodings_to_dict(seat_data, et1, et2, ea1, ea2, suppress_printables = False)\n",
        "    run_test(seat_data, 10000, suppress_printables = False) # X, Y need to have same length for p_val permutation test\n",
        "    print(f\"{seat_data['targ1']['category']} vs {seat_data['targ2']['category']} ({seat_data['attr1']['category']} / {seat_data['attr2']['category']})\")"
      ],
      "metadata": {
        "id": "B3yQyWXHW7uA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}